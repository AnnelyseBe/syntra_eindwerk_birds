{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 10919...\n",
      "Scraping page 10920...\n",
      "Scraping page 10921...\n",
      "Scraping page 10922...\n",
      "Scraping page 10923...\n",
      "Scraping page 10924...\n",
      "Scraping page 10925...\n",
      "Scraping page 10926...\n",
      "Scraping page 10927...\n",
      "Scraping page 10928...\n",
      "Scraping page 10929...\n",
      "Scraping page 10930...\n",
      "Scraping page 10931...\n",
      "Scraping page 10932...\n",
      "Scraping page 10933...\n",
      "Scraping page 10934...\n",
      "Scraping page 10935...\n",
      "Scraping page 10936...\n",
      "Scraping page 10937...\n",
      "Scraping page 10938...\n",
      "Scraping page 10939...\n",
      "Scraping page 10940...\n",
      "Scraping page 10941...\n",
      "Scraping page 10942...\n",
      "Scraping page 10943...\n",
      "Scraping page 10944...\n",
      "Scraping page 10945...\n",
      "Scraping page 10946...\n",
      "Scraping page 10947...\n",
      "Scraping page 10948...\n",
      "Scraping page 10949...\n",
      "Scraping page 10950...\n",
      "Scraping page 10951...\n",
      "Scraping page 10952...\n",
      "Scraping page 10953...\n",
      "Scraping page 10954...\n",
      "Scraping page 10955...\n",
      "Scraping page 10956...\n",
      "Scraping page 10957...\n",
      "Scraping page 10958...\n",
      "Scraping page 10959...\n",
      "Scraping page 10960...\n",
      "Scraping page 10961...\n",
      "Scraping page 10962...\n",
      "Scraping page 10963...\n",
      "Scraping page 10964...\n",
      "Scraping page 10965...\n",
      "Scraping page 10966...\n",
      "Scraping page 10967...\n",
      "Scraping page 10968...\n",
      "Scraping page 10969...\n",
      "Scraping page 10970...\n",
      "Scraping page 10971...\n",
      "Scraping page 10972...\n",
      "Scraping page 10973...\n",
      "Scraping page 10974...\n",
      "Scraping page 10975...\n",
      "Scraping page 10976...\n",
      "Scraping page 10977...\n",
      "Scraping page 10978...\n",
      "Scraping page 10979...\n",
      "Scraping page 10980...\n",
      "Scraping page 10981...\n",
      "Scraping page 10982...\n",
      "Scraping page 10983...\n",
      "Scraping page 10984...\n",
      "Scraping page 10985...\n",
      "Scraping page 10986...\n",
      "Scraping page 10987...\n",
      "Scraping page 10988...\n",
      "Scraping page 10989...\n",
      "Scraping page 10990...\n",
      "Scraping page 10991...\n",
      "Scraping page 10992...\n",
      "Scraping page 10993...\n",
      "Scraping page 10994...\n",
      "Scraping page 10995...\n",
      "Scraping page 10996...\n",
      "Scraping page 10997...\n",
      "Scraping page 10998...\n",
      "Scraping page 10999...\n",
      "Scraping page 11000...\n",
      "Scraping page 11001...\n",
      "Scraping page 11002...\n",
      "Scraping page 11003...\n",
      "Scraping page 11004...\n",
      "Scraping page 11005...\n",
      "Scraping page 11006...\n",
      "Scraping page 11007...\n",
      "Scraping page 11008...\n",
      "Scraping page 11009...\n",
      "Scraping page 11010...\n",
      "Scraping page 11011...\n",
      "Scraping page 11012...\n",
      "Scraping page 11013...\n",
      "Scraping page 11014...\n",
      "Scraping page 11015...\n",
      "Scraping page 11016...\n",
      "Scraping page 11017...\n",
      "Scraping page 11018...\n",
      "Scraping page 11019...\n",
      "Scraping page 11020...\n",
      "Scraping page 11021...\n",
      "Scraping page 11022...\n",
      "Scraping page 11023...\n",
      "Scraping page 11024...\n",
      "Scraping page 11025...\n",
      "Scraping page 11026...\n",
      "Scraping page 11027...\n",
      "Scraping page 11028...\n",
      "Scraping page 11029...\n",
      "Scraping page 11030...\n",
      "Scraping page 11031...\n",
      "Scraping page 11032...\n",
      "Scraping page 11033...\n",
      "Scraping page 11034...\n",
      "Scraping page 11035...\n",
      "Scraping page 11036...\n",
      "Scraping page 11037...\n",
      "Scraping page 11038...\n",
      "Scraping page 11039...\n",
      "Scraping page 11040...\n",
      "Scraping page 11041...\n",
      "Scraping page 11042...\n",
      "Scraping page 11043...\n",
      "Scraping page 11044...\n",
      "Scraping page 11045...\n",
      "Scraping page 11046...\n",
      "Scraping page 11047...\n",
      "Scraping page 11048...\n",
      "Scraping page 11049...\n",
      "Scraping page 11050...\n",
      "Scraping page 11051...\n",
      "Scraping page 11052...\n",
      "Scraping page 11053...\n",
      "Scraping page 11054...\n",
      "Scraping page 11055...\n",
      "Scraping page 11056...\n",
      "Scraping page 11057...\n",
      "Scraping page 11058...\n",
      "Scraping page 11059...\n",
      "Scraping page 11060...\n",
      "Scraping page 11061...\n",
      "Scraping page 11062...\n",
      "Scraping page 11063...\n",
      "Scraping page 11064...\n",
      "Scraping page 11065...\n",
      "Scraping page 11066...\n",
      "Scraping page 11067...\n",
      "Scraping page 11068...\n",
      "Scraping page 11069...\n",
      "Scraping page 11070...\n",
      "Scraping page 11071...\n",
      "Scraping page 11072...\n",
      "Scraping page 11073...\n",
      "Scraping page 11074...\n",
      "Scraping page 11075...\n",
      "Scraping page 11076...\n",
      "Scraping page 11077...\n",
      "Scraping page 11078...\n",
      "Scraping page 11079...\n",
      "Scraping page 11080...\n",
      "Scraping page 11081...\n",
      "Scraping page 11082...\n",
      "Scraped 4082 observations, last scraped page: 11082\n",
      "Scraping complete. Data saved to .csv file\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "species = {70: \"boomklever\", 77: \"bosuil\", 116: \"halsbandparkiet\", 300: \"oehoe\", 362: \"zwarte ooievaar\", 10041: \"gewone wasbeer\"}\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/species/{}/observations/?date_after=1900-01-01&date_before=2024-12-31&page={}'  # te verwachten tot page 3879\n",
    "\n",
    "# Function to parse a single page\n",
    "def parse_page(species_id, page_number, retries=10, backoff_factor=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = requests.get(base_url.format(species_id, page_number))\n",
    "            res.raise_for_status()\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            # Extract table rows\n",
    "            rows = soup.select('table tbody tr')\n",
    "            observations = []\n",
    "            \n",
    "            for row in rows:\n",
    "                cols = row.find_all('td')\n",
    "                if len(cols) >= 5:\n",
    "                    observation = {\n",
    "                        'date': cols[0].getText().strip(),\n",
    "                        'id': cols[0].a['href'].strip('/observation/').strip('/') if cols[0].a else None,\n",
    "                        'amount_manner': cols[1].getText().strip(),\n",
    "                        'location': cols[2].getText().strip(),\n",
    "                        'location_id': cols[2].a['href'].strip('/locations/').strip('/') if cols[2].a else None,\n",
    "                        'observer_name': cols[3].getText().strip(),\n",
    "                        'observer_id': cols[3].a['href'].strip('/users/').strip('/') if cols[3].a else None,\n",
    "                        'validation': cols[4].i['title'].strip() if cols[4].i else None,\n",
    "                        'page' : page_number\n",
    "                    }\n",
    "                    observations.append(observation)\n",
    "                    \n",
    "            pagination = soup.select('.pagination li')\n",
    "\n",
    "            has_next_page = False\n",
    "            for page in pagination:\n",
    "                text = page.getText()\n",
    "                try:\n",
    "                    number = int(text)\n",
    "                    if number > page_number:\n",
    "                        has_next_page = True\n",
    "                        break\n",
    "                except ValueError:\n",
    "                    continue\n",
    "            return observations, has_next_page\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error: {e} on attempt {attempt + 1}/{retries}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(backoff_factor * (2 ** attempt))\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "def scrape(species_id, page_start = 1, page_end_incl = 100_000):\n",
    "    file_name = make_filename(species_id)\n",
    "    page = page_start\n",
    "    keep_scraping = True\n",
    "    first_write = True if page_start == 1 else False  # Add header only for the first page\n",
    "    \n",
    "    while keep_scraping:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        try:\n",
    "            observations, has_next_page = parse_page(species_id, page)\n",
    "            if not observations or not has_next_page or page >= page_end_incl:\n",
    "                keep_scraping = False\n",
    "            \n",
    "            # Convert observations to a DataFrame\n",
    "            if observations:\n",
    "                df = pd.DataFrame(observations)\n",
    "                \n",
    "                # Append observations to CSV\n",
    "                df.to_csv(\n",
    "                    file_name,\n",
    "                    mode='a',  # Append mode\n",
    "                    index=False,\n",
    "                    header=first_write  # Write header only for the first write\n",
    "                )\n",
    "                first_write = False  # Ensure header is only written once\n",
    "            \n",
    "            time.sleep(5)  # Respectful delay between requests\n",
    "            page += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "    df = pd.read_csv(file_name)\n",
    "    print(f\"Scraped {len(df)} observations, last scraped page: {page - 1}\")\n",
    "    os.rename(file_name, make_filename(species_id, page_start, page - 1))\n",
    "    return df\n",
    "\n",
    "def make_filename(species_id, page_start = None, page_end_incl = None):\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    base_name = f'scraped_data/observations_{species_id}_{current_date}'\n",
    "    if page_start and page_end_incl:\n",
    "        base_name = base_name + f'_pages_{page_start}-{page_end_incl}'\n",
    "    return base_name + '.csv'\n",
    "\n",
    "#################################################################################################################\n",
    "\n",
    "species_id = 70\n",
    "page_start = 10919\n",
    "page_end_incl = 2\n",
    "\n",
    "# scrape(species_id) # Scrape all pages\n",
    "scrape(species_id, page_start) # Scrape all pages beginning from \n",
    "# scrape(species_id, page_start, page_end_incl) # Scrape section\n",
    "\n",
    "print(\"Scraping complete. Data saved to .csv file\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
