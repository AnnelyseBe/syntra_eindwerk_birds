{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping complete. Data saved to 'observations.csv'\n",
      "               date         id              amount_manner  \\\n",
      "0  2025-01-14 16:57  337060073  2  overvliegend noordwest   \n",
      "1  2025-01-14 16:33  337053960                         10   \n",
      "2        2025-01-14  337050655                          1   \n",
      "3  2025-01-14 14:35  337054880             1  foeragerend   \n",
      "4  2025-01-14 13:38  337053880       3  gezien en gehoord   \n",
      "\n",
      "                                            location location_id  \\\n",
      "0           Deerlijk - De Bonte Os (West-Vlaanderen)      623755   \n",
      "1                   Diest - Centrum (Vlaams-Brabant)       30991   \n",
      "2  Harelbeke/Deerlijk - De Gavers (Provinciedomei...       31857   \n",
      "3                Diest - Halve Maan (Vlaams-Brabant)       92164   \n",
      "4           Hofstade - BLOSO-Domein (Vlaams-Brabant)       30871   \n",
      "\n",
      "  observer_name observer_id                            validation  \n",
      "0    yann feryn       40457                              onbekend  \n",
      "1   luc cieters       43164  goedgekeurd (automatische validatie)  \n",
      "2         Bytte      391063  goedgekeurd (automatische validatie)  \n",
      "3  Kamiel Aerts       41257  goedgekeurd (automatische validatie)  \n",
      "4  Lore De Wolf      167953  goedgekeurd (automatische validatie)  \n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/observation/{}'\n",
    "\n",
    "# Function to parse a single observation\n",
    "def parse_page(observation_id):\n",
    "    res = requests.get(base_url.format(observation_id))\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = soup.select('table tbody tr')\n",
    "    observations = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) >= 5:\n",
    "            observation = {\n",
    "                'date': cols[0].getText().strip(),\n",
    "                'id': cols[0].a['href'].strip('/observation/').strip('/') if cols[0].a else None,\n",
    "                'amount_manner': cols[1].getText().strip(),\n",
    "                'location': cols[2].getText().strip(),\n",
    "                'location_id': cols[2].a['href'].strip('/locations/').strip('/') if cols[2].a else None,\n",
    "                'observer_name': cols[3].getText().strip(),\n",
    "                'observer_id': cols[3].a['href'].strip('/users/').strip('/') if cols[3].a else None,\n",
    "                'validation': cols[4].i['title'].strip() if cols[4].i else None,\n",
    "            }\n",
    "            observations.append(observation)\n",
    "    \n",
    "    return observations\n",
    "\n",
    "def scrape_all_pages(species_id):\n",
    "    all_observations = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        observations = parse_page(species_id, page)\n",
    "        if not observations:  # Stop if no more data\n",
    "            break\n",
    "        all_observations.extend(observations)\n",
    "        page += 1\n",
    "    return all_observations\n",
    "\n",
    "def scrape_multiple_pages(species_id, page_start, page_end_excl):\n",
    "    all_observations = []\n",
    "    for page in range(page_start, page_end_excl): \n",
    "        print(f\"Scraping page {page}...\")\n",
    "        observations = parse_page(species_id, page)\n",
    "        if not observations:  # Stop if no more data\n",
    "            break\n",
    "        all_observations.extend(observations)\n",
    "    return all_observations\n",
    "\n",
    "def scrape(species_id, page_start=0, page_end_excl=0):\n",
    "    if page_end_excl == 0:\n",
    "        return scrape_all_pages(species_id)\n",
    "    else:\n",
    "        return scrape_multiple_pages(species_id, page_start, page_end_excl)\n",
    "\n",
    "\n",
    "species_id = 116\n",
    "page_start = 1\n",
    "page_end_excl = 4\n",
    "\n",
    "# observations = scrape_multiple_pages(species_id, page_start, page_end_excl)\n",
    "observations = scrape(species_id, page_start, page_end_excl)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(observations)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "if page_start == 0 and page_end_excl == 0:\n",
    "    df.to_csv(f'observations_{species_id}_{current_date}_full.csv', index=False)\n",
    "else :\n",
    "    df.to_csv(f'observations_{species_id}_{current_date}_{page_start}_{page_end_excl}.csv', index=False)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'observations.csv'\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
