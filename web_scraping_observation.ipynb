{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/species/{}/observations/?date_after=1900-01-01&date_before=2024-12-31&page={}'  # te verwachten tot page 3879\n",
    "\n",
    "# Function to parse a single page\n",
    "def parse_observation(observation_id, retries=10, backoff_factor=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = requests.get(base_url.format(observation_id))\n",
    "            res.raise_for_status()\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            observation = {\n",
    "                \"species_id\": soup.select_one(\"h1 > a\").get('href').strip('/species/').strip('/observations/'),\n",
    "                \"species_name\": soup.select_one(\".species-common-name\").getText().strip(),\n",
    "                \"species_name_scientific\": soup.select_one(\".species-scientific-name\").getText().strip(),\n",
    "                \n",
    "                \"validation\": soup.select_one(\".validation-status-text\").getText().strip(),\n",
    "                \n",
    "                \"gps_coordinates\": soup.select_one('span[title=\"WGS 84\"] .teramap-coordinates-coords').getText().strip(),\n",
    "                \"accuracy\": soup.find('span', string=\"Nauwkeurigheid\").next_sibling.getText().strip(),\n",
    "                \"source\": soup.find('span', string=\"Bron\").next_sibling.getText().strip(),\n",
    "                \n",
    "                \"date\": soup.find(\"#observation_details th\", string=\"Datum\").next_sibling.getText().strip()#datum\n",
    "                # \"amount\":#aantal\n",
    "                # \"life_stage\":#levensstadium\n",
    "                # \"activity\":#activiteit\n",
    "                # \"location_id\": soup.select_one('tr > th').a.get('href').strip('/locations/').strip('/'), #######################todo hier zitten we, gebaseerd op https://waarnemingen.be/observation/336664701/\n",
    "                # \"location\": #locatie\n",
    "                # 'observer_name': cols[3].getText().strip(),\n",
    "                # 'observer_id': cols[3].a['href'].strip('/users/').strip('/') if cols[3].a else None,\n",
    "                # \"counting_method\":#telmethode\n",
    "                # \"method\":#methode\n",
    "                    \n",
    "                \n",
    "            }\n",
    "            return observation\n",
    "                    \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP error: {e} on attempt {attempt + 1}/{retries}\")\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(backoff_factor * (2 ** attempt))\n",
    "                continue\n",
    "            else:\n",
    "                raise e\n",
    "            \n",
    "def scrape(observations = [\"336664701\"]):\n",
    "    file_name = make_filename(10041)\n",
    "    \n",
    "    try:\n",
    "        for i, observation_id in enumerate(observations):\n",
    "            if (i == 0 and not os.path.isfile(file_name)):\n",
    "                write_header = True\n",
    "            else:\n",
    "                write_header = False\n",
    "            \n",
    "            observation = parse_observation(observation_id)\n",
    "            pd.DataFrame(observation).to_csv(\n",
    "                file_name,\n",
    "                mode='a',  # Append mode\n",
    "                index=False,\n",
    "                header=write_header  # Write header only for the first write in the file\n",
    "            )\n",
    "            \n",
    "            time.sleep(5)  # Respectful delay between requests\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        break\n",
    "\n",
    "\n",
    "def make_filename(species_id):\n",
    "    current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    base_name = f'scraped_data/observation_details_{species_id}_{current_date}'\n",
    "    return base_name + '.csv'\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping complete. Data saved to 'observations.csv'\n",
      "               date         id              amount_manner  \\\n",
      "0  2025-01-14 16:57  337060073  2  overvliegend noordwest   \n",
      "1  2025-01-14 16:33  337053960                         10   \n",
      "2        2025-01-14  337050655                          1   \n",
      "3  2025-01-14 14:35  337054880             1  foeragerend   \n",
      "4  2025-01-14 13:38  337053880       3  gezien en gehoord   \n",
      "\n",
      "                                            location location_id  \\\n",
      "0           Deerlijk - De Bonte Os (West-Vlaanderen)      623755   \n",
      "1                   Diest - Centrum (Vlaams-Brabant)       30991   \n",
      "2  Harelbeke/Deerlijk - De Gavers (Provinciedomei...       31857   \n",
      "3                Diest - Halve Maan (Vlaams-Brabant)       92164   \n",
      "4           Hofstade - BLOSO-Domein (Vlaams-Brabant)       30871   \n",
      "\n",
      "  observer_name observer_id                            validation  \n",
      "0    yann feryn       40457                              onbekend  \n",
      "1   luc cieters       43164  goedgekeurd (automatische validatie)  \n",
      "2         Bytte      391063  goedgekeurd (automatische validatie)  \n",
      "3  Kamiel Aerts       41257  goedgekeurd (automatische validatie)  \n",
      "4  Lore De Wolf      167953  goedgekeurd (automatische validatie)  \n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/observation/{}'\n",
    "\n",
    "# Function to parse a single observation\n",
    "def parse_observation(observation_id):\n",
    "    res = requests.get(base_url.format(observation_id))\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "    \n",
    "    # Extract table rows\n",
    "    rows = soup.select('table tbody tr')\n",
    "    observations = []\n",
    "    \n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) >= 5:\n",
    "            observation = {\n",
    "                'date': cols[0].getText().strip(),\n",
    "                'id': cols[0].a['href'].strip('/observation/').strip('/') if cols[0].a else None,\n",
    "                'amount_manner': cols[1].getText().strip(),\n",
    "                'location': cols[2].getText().strip(),\n",
    "                'location_id': cols[2].a['href'].strip('/locations/').strip('/') if cols[2].a else None,\n",
    "                'observer_name': cols[3].getText().strip(),\n",
    "                'observer_id': cols[3].a['href'].strip('/users/').strip('/') if cols[3].a else None,\n",
    "                'validation': cols[4].i['title'].strip() if cols[4].i else None,\n",
    "            }\n",
    "            observations.append(observation)\n",
    "    \n",
    "    return observations\n",
    "\n",
    "def scrape_all_pages(species_id):\n",
    "    all_observations = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        observations = parse_observation(species_id, page)\n",
    "        if not observations:  # Stop if no more data\n",
    "            break\n",
    "        all_observations.extend(observations)\n",
    "        page += 1\n",
    "    return all_observations\n",
    "\n",
    "def scrape_multiple_pages(species_id, page_start, page_end_excl):\n",
    "    all_observations = []\n",
    "    for page in range(page_start, page_end_excl): \n",
    "        print(f\"Scraping page {page}...\")\n",
    "        observations = parse_observation(species_id, page)\n",
    "        if not observations:  # Stop if no more data\n",
    "            break\n",
    "        all_observations.extend(observations)\n",
    "    return all_observations\n",
    "\n",
    "def scrape(species_id, page_start=0, page_end_excl=0):\n",
    "    if page_end_excl == 0:\n",
    "        return scrape_all_pages(species_id)\n",
    "    else:\n",
    "        return scrape_multiple_pages(species_id, page_start, page_end_excl)\n",
    "\n",
    "\n",
    "species_id = 116\n",
    "page_start = 1\n",
    "page_end_excl = 4\n",
    "\n",
    "# observations = scrape_multiple_pages(species_id, page_start, page_end_excl)\n",
    "observations = scrape(species_id, page_start, page_end_excl)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(observations)\n",
    "\n",
    "# Save to CSV (optional)\n",
    "current_date = datetime.now().strftime('%Y-%m-%d')\n",
    "if page_start == 0 and page_end_excl == 0:\n",
    "    df.to_csv(f'observations_{species_id}_{current_date}_full.csv', index=False)\n",
    "else :\n",
    "    df.to_csv(f'observations_{species_id}_{current_date}_{page_start}_{page_end_excl}.csv', index=False)\n",
    "\n",
    "print(\"Scraping complete. Data saved to 'observations.csv'\")\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
