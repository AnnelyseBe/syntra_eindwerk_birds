{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start scraping: 10000 this batch, but 92470 observations to scrape\n",
      "Scraping observation 322156271 (1/10000)\n",
      "Requesting https://waarnemingen.be/observation/322156271/ (Attempt 1)\n",
      "Observation 322156271 not found (404). Returning empty observation.\n",
      "Scraping observation 322149999 (2/10000)\n",
      "Requesting https://waarnemingen.be/observation/322149999/ (Attempt 1)\n",
      "Scraping observation 322104306 (3/10000)\n",
      "Requesting https://waarnemingen.be/observation/322104306/ (Attempt 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 140\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(not_in_details) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart scraping: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscrape_amount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m this batch, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(not_in_details)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m observations to scrape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 140\u001b[0m     \u001b[43mscrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnot_in_details\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mscrape_amount\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecies_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path_clean\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll observations have been scraped.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mscrape\u001b[0;34m(observations, species_id, folder_path)\u001b[0m\n\u001b[1;32m     98\u001b[0m         observation \u001b[38;5;241m=\u001b[39m parse_observation(observation_id)\n\u001b[1;32m     99\u001b[0m         pd\u001b[38;5;241m.\u001b[39mDataFrame([observation])\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[1;32m    100\u001b[0m             file_name,\n\u001b[1;32m    101\u001b[0m             mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Append mode\u001b[39;00m\n\u001b[1;32m    102\u001b[0m             index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m             header\u001b[38;5;241m=\u001b[39mwrite_header  \u001b[38;5;66;03m# Write header only for the first write in the file\u001b[39;00m\n\u001b[1;32m    104\u001b[0m         )\n\u001b[0;32m--> 106\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Variable respectful delay between requests\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/observation/{}/'  \n",
    "\n",
    "# Function to parse a single page\n",
    "def parse_observation(observation_id, retries=10, backoff_factor=2):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-encoding\":\"gzip, deflate, br, zstd\",\n",
    "        \"accept-language\":\"nl-BE,nl;q=0.9,en-BE;q=0.8,en;q=0.7,nl-NL;q=0.6,en-US;q=0.5\",\n",
    "        \"connection\":\"keep-alive\",\n",
    "        \"cookie\":\"csrftoken=3JbFPYJyRC9GxhkNoW4XzF1vbbG6Fbxe; sessionid=v132os9mxwltj3ol3plhmojrjch24m9o; fundraiser_dismissed=1; cookielaw_accepted=1\",\n",
    "        \"host\":\"waarnemingen.be\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"sec-ch-ua\":'\"Not A(Brand\";v=\"8\", \"Chromium\";v=\"132\", \"Google Chrome\";v=\"132\"',\n",
    "        \"sec-ch-ua-mobile\":\"?0\",\n",
    "        \"sec-ch-ua-platform\":\"Linux\",\n",
    "        \"sec-fetch-dest\":\"document\",\n",
    "        \"sec-fetch-mode\":\"navigate\",\n",
    "        \"sec-fetch-site\":\"same-origin\",\n",
    "        \"sec-fetch-user\":\"?1\",\n",
    "        \"upgrade-insecure-requests\":\"1\"\n",
    "            }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = requests.get(base_url.format(observation_id), headers=headers)\n",
    "            print(f\"Requesting {base_url.format(observation_id)} (Attempt {attempt + 1})\")\n",
    "            res.raise_for_status()\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            observation = {\n",
    "                \"observation_id\": observation_id,\n",
    "                \"species_id\": soup.select_one(\"h1 > a\").get('href').split('/')[2],\n",
    "                \"species_name\": soup.select_one(\".species-common-name\").getText().strip(),\n",
    "                \"species_name_scientific\": soup.select_one(\".species-scientific-name\").getText().strip(),\n",
    "                \"validation\": soup.select_one(\".validation-status-text\").getText().strip(),\n",
    "                \"gps_coordinates\": soup.select_one('span[title=\"WGS 84\"] .teramap-coordinates-coords').getText().strip() if soup.select_one('span[title=\"WGS 84\"] .teramap-coordinates-coords') else None,\n",
    "                \"accuracy\": soup.find('span', string=\"Nauwkeurigheid\").next_sibling.getText().strip() if soup.find('span', string=\"Nauwkeurigheid\") else None,\n",
    "                \"source\": soup.find('span', string=\"Bron\").next_sibling.getText().strip() if soup.find('span', string=\"Bron\") else None,\n",
    "                \"date\": soup.select_one('th:-soup-contains(\"Datum\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Datum\") + td') else None,\n",
    "                \"amount\": soup.select_one('th:-soup-contains(\"Aantal\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Aantal\") + td') else None, # aantal\n",
    "                \"life_stage\": soup.select_one('th:-soup-contains(\"Levensstadium\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Levensstadium\") + td') else None, # levensstadium\n",
    "                \"activity\": soup.select_one('th:-soup-contains(\"Activiteit\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Activiteit\") + td') else None, # activiteit\n",
    "   \n",
    "                \"location_id\": soup.select_one('th:-soup-contains(\"Locatie\") + td').a.get('href').split('/')[2] if soup.select_one('th:-soup-contains(\"Locatie\") + td a') else None, \n",
    "                \"location\": soup.select_one('th:-soup-contains(\"Locatie\") + td a').getText().strip() if soup.select_one('th:-soup-contains(\"Locatie\") + td a') else None, # locatie\n",
    "                \"observer_id\": soup.select_one('th:-soup-contains(\"Waarnemer\") + td').a.get('href').split('/')[2] if soup.select_one('th:-soup-contains(\"Waarnemer\") + td a') else None,\n",
    "                \"observer_name\": soup.select_one('th:-soup-contains(\"Waarnemer\") + td a').getText().strip() if soup.select_one('th:-soup-contains(\"Waarnemer\") + td a') else None, # waarnemer\n",
    "                \n",
    "                \"counting_method\": soup.select_one('th:-soup-contains(\"Telmethode\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Telmethode\") + td') else None, # telmethode\n",
    "                \"method\": soup.select_one('th:-soup-contains(\"Methode\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Methode\") + td') else None, # methode\n",
    "            }\n",
    "            # print(observation)\n",
    "            return observation\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if res.status_code == 404:\n",
    "                print(f\"Observation {observation_id} not found (404). Returning empty observation.\")\n",
    "                return {\"observation_id\": observation_id, **{key: None for key in [\n",
    "                    \"species_id\", \"species_name\", \"species_name_scientific\", \"validation\",\n",
    "                    \"gps_coordinates\", \"accuracy\", \"source\", \"date\", \"amount\",\n",
    "                    \"life_stage\", \"activity\", \"location_id\", \"location\",\n",
    "                    \"observer_id\", \"observer_name\", \"counting_method\", \"method\"\n",
    "                ]}}\n",
    "            else:\n",
    "                print(f\"HTTP error {res.status_code} on attempt {attempt + 1}/{retries}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e} on attempt {attempt + 1}/{retries}\")\n",
    "            \n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(backoff_factor * (2 ** attempt))\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Failed to fetch observation {observation_id} after {retries} attempts.\")\n",
    "            raise e\n",
    "            \n",
    "def scrape(observations, species_id, folder_path):\n",
    "    file_name = make_filename(species_id, folder_path)\n",
    "    # print(f\"make_filename: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        for i, observation_id in enumerate(observations):\n",
    "            print(f\"Scraping observation {observation_id} ({i+1}/{len(observations)})\")\n",
    "            if (i == 0 and not os.path.isfile(file_name)):\n",
    "                write_header = True\n",
    "                # print(f\"Writing header to {file_name}\")\n",
    "            else:\n",
    "                write_header = False\n",
    "            \n",
    "            observation = parse_observation(observation_id)\n",
    "            pd.DataFrame([observation]).to_csv(\n",
    "                file_name,\n",
    "                mode='a',  # Append mode\n",
    "                index=False,\n",
    "                header=write_header  # Write header only for the first write in the file\n",
    "            )\n",
    "            \n",
    "            time.sleep(randint(2,10))  # Variable respectful delay between requests\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "def make_filename(species_id, folder_path, complete=False):\n",
    "    os.makedirs('scraped_data', exist_ok=True)\n",
    "    base_name = f'{folder_path}/observation_details_{species_id}_clean'\n",
    "    if not complete:\n",
    "        return base_name + '_in_progress.csv'\n",
    "    else:\n",
    "        return base_name + '.csv'\n",
    "\n",
    "###########################################################################################################################################\n",
    "species_id = 116\n",
    "folder_path_clean = \"./scraped_data/cleaned/\"\n",
    "scrape_amount = 10000\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_general = pd.read_csv(folder_path_clean + \"observations_general_\" + str(species_id) + \"_clean\" + \".csv\")\n",
    "details = [f for f in os.listdir(folder_path_clean) if f\"observation_details_{species_id}_clean\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "df_details = pd.read_csv(folder_path_clean + details[0]) if len(details) > 0 else None\n",
    "\n",
    "if (df_details is not None):\n",
    "    # Find ids in df_general that are not in df_details\n",
    "    not_in_details = df_general[~df_general['id'].isin(df_details['observation_id'])]['id'].tolist()\n",
    "else:\n",
    "    not_in_details = df_general['id'].tolist()\n",
    "    \n",
    "\n",
    "if len(not_in_details) > 0:\n",
    "    print(f\"Start scraping: {scrape_amount} this batch, but {len(not_in_details)} observations to scrape\")\n",
    "    scrape(not_in_details[0:scrape_amount], species_id, folder_path_clean)\n",
    "else:\n",
    "    print(\"All observations have been scraped.\")\n",
    "    os.rename(make_filename(species_id, folder_path_clean, complete=False), make_filename(species_id, folder_path_clean, complete=True))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "days, rem = divmod(execution_time.total_seconds(), 86400)\n",
    "hours, rem = divmod(rem, 3600)\n",
    "minutes, rem = divmod(rem, 60)\n",
    "seconds, _ = divmod(rem, 1)\n",
    "print(f\"Execution time: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
