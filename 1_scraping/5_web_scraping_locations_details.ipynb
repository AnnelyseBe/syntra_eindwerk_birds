{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapen van de individuele locaties\n",
    "\n",
    "- Doel: het verkrijgen van de oppervlakte van elke locatie\n",
    "- Scrapen\n",
    "  - Een variabele, maar respectvolle tijd voorzien tussen 2 aanvragen\n",
    "  - Voldoende tijd te voorzien\n",
    "- Op basis van de lijst met alle locatie id's per soort, kunnen we onze csv file telkens aanvullen met ontbrekende area. Zo kunnen we de data ophalen in verschillende sessies en bij elke scrape-sessie aanvullen met de observaties die we ontbreken. Van zodra alle observaties opgehaald zijn, verandert de filename van _in_progress.csv to _complete.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All locations have been scraped.\n",
      "Execution time: 0 days, 0 hours, 0 minutes, 0 seconds\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/locations/{}/'  \n",
    "\n",
    "# Function to parse a single location\n",
    "def parse_location(location_id, retries=10, backoff_factor=2):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-encoding\":\"gzip, deflate, br, zstd\",\n",
    "        \"accept-language\":\"nl-BE,nl;q=0.9,en-BE;q=0.8,en;q=0.7,nl-NL;q=0.6,en-US;q=0.5\",\n",
    "        \"connection\":\"keep-alive\",\n",
    "        \"cookie\":\"csrftoken=3JbFPYJyRC9GxhkNoW4XzF1vbbG6Fbxe; sessionid=v132os9mxwltj3ol3plhmojrjch24m9o; fundraiser_dismissed=1; cookielaw_accepted=1\",\n",
    "        \"host\":\"waarnemingen.be\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"sec-ch-ua\":'\"Not A(Brand\";v=\"8\", \"Chromium\";v=\"132\", \"Google Chrome\";v=\"132\"',\n",
    "        \"sec-ch-ua-mobile\":\"?0\",\n",
    "        \"sec-ch-ua-platform\":\"Linux\",\n",
    "        \"sec-fetch-dest\":\"document\",\n",
    "        \"sec-fetch-mode\":\"navigate\",\n",
    "        \"sec-fetch-site\":\"same-origin\",\n",
    "        \"sec-fetch-user\":\"?1\",\n",
    "        \"upgrade-insecure-requests\":\"1\"\n",
    "            }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = requests.get(base_url.format(location_id), headers=headers)\n",
    "            print(f\"Requesting {base_url.format(location_id)} (Attempt {attempt + 1})\")\n",
    "            res.raise_for_status()\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            area = soup.select_one('th:-soup-contains(\"Oppervlakte\") + td').get_text(strip=True) if soup.select_one('th:-soup-contains(\"Oppervlakte\") + td') else None # oppervlakte\n",
    "            return area\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if res.status_code == 404:\n",
    "                print(f\"Location {location_id} not found (404). Returning empty location.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"HTTP error {res.status_code} on attempt {attempt + 1}/{retries}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e} on attempt {attempt + 1}/{retries}\")\n",
    "            \n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(backoff_factor * (2 ** attempt))\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Failed to fetch location {location_id} after {retries} attempts.\")\n",
    "            raise e\n",
    "            \n",
    "def scrape(locations, folder_path, sleep_min = 2, sleep_max = 10):\n",
    "    file_name = make_filename(folder_path)\n",
    "    print(f\"Scraping {locations.shape[0]} locations to {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        for i, location in locations.iterrows():\n",
    "            print(f\"Scraping location {location[\"location_id\"]} | ({i+1:_}/{locations.shape[0]:_})\")\n",
    "            if (i == 0 and not os.path.isfile(file_name)):\n",
    "                write_header = True\n",
    "            else:\n",
    "                write_header = False\n",
    "            \n",
    "            location['area'] = parse_location(location[\"location_id\"])\n",
    "\n",
    "            \n",
    "            location_df = pd.DataFrame([location])\n",
    "            location_df.to_csv(\n",
    "                file_name,\n",
    "                mode='a',  # Append mode\n",
    "                index=False,\n",
    "                header=write_header  # Write header only for the first write in the file\n",
    "            )\n",
    "            \n",
    "            time.sleep(randint(sleep_min,sleep_max))  # Variable respectful delay between requests\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        location_df = pd.DataFrame([location])\n",
    "def make_filename(folder_path, complete=False):\n",
    "    os.makedirs('scraped_data', exist_ok=True)\n",
    "    base_name = f'{folder_path}/location_details'\n",
    "    if not complete:\n",
    "        return base_name + '_in_progress.csv'\n",
    "    else:\n",
    "        return base_name + '_complete.csv'\n",
    "\n",
    "###########################################################################################################################################\n",
    "folder_path_clean = \"./scraped_data/cleaned/\"\n",
    "folder_path_raw = \"./scraped_data/\"\n",
    "scrape_amount = 100000\n",
    "sleep_min = 1\n",
    "sleep_max = 4\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_general = pd.read_csv(folder_path_clean + \"locations_general_clean.csv\") # source\n",
    "details = [f for f in os.listdir(folder_path_raw) if f\"location_details\" in f and f.endswith(\".csv\")] # target in progress\n",
    "\n",
    "df_details = pd.read_csv(folder_path_raw + details[0]) if len(details) > 0 else None\n",
    "\n",
    "if (df_details is not None):\n",
    "    # Find ids in df_general that are not in df_details\n",
    "    df_to_scrape = df_general[~df_general['location_id'].isin(df_details['location_id'])]\n",
    "else:\n",
    "    df_to_scrape = df_general\n",
    "    \n",
    "\n",
    "if df_to_scrape.shape[0] > 0:\n",
    "    print(f\"Start scraping: {scrape_amount:_} this batch, {df_to_scrape.shape[0]:_} locations to scrape\")\n",
    "    scrape(df_to_scrape.iloc[0:scrape_amount,:], folder_path_raw, sleep_min, sleep_max)\n",
    "else:\n",
    "    print(\"All locations have been scraped.\")\n",
    "    if 'in_progress' in details[0]:\n",
    "        os.rename(make_filename(folder_path_raw, complete=False), make_filename(folder_path_raw, complete=True))\n",
    "        \n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "days, rem = divmod(execution_time.total_seconds(), 86400)\n",
    "hours, rem = divmod(rem, 3600)\n",
    "minutes, rem = divmod(rem, 60)\n",
    "seconds, _ = divmod(rem, 1)\n",
    "print(f\"Execution time: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
