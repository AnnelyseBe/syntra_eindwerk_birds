{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All observations have been scraped.\n",
      "Execution time: 0 days, 0 hours, 0 minutes, 3 seconds\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from random import randint\n",
    "\n",
    "# Base URL for scraping\n",
    "base_url = 'https://waarnemingen.be/observation/{}/'  \n",
    "\n",
    "# Function to parse a single page\n",
    "def parse_observation(observation_id, retries=10, backoff_factor=2):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36\",\n",
    "        \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "        \"accept-encoding\":\"gzip, deflate, br, zstd\",\n",
    "        \"accept-language\":\"nl-BE,nl;q=0.9,en-BE;q=0.8,en;q=0.7,nl-NL;q=0.6,en-US;q=0.5\",\n",
    "        \"connection\":\"keep-alive\",\n",
    "        \"cookie\":\"csrftoken=3JbFPYJyRC9GxhkNoW4XzF1vbbG6Fbxe; sessionid=v132os9mxwltj3ol3plhmojrjch24m9o; fundraiser_dismissed=1; cookielaw_accepted=1\",\n",
    "        \"host\":\"waarnemingen.be\",\n",
    "        \"Referer\": \"https://www.google.com/\",\n",
    "        \"sec-ch-ua\":'\"Not A(Brand\";v=\"8\", \"Chromium\";v=\"132\", \"Google Chrome\";v=\"132\"',\n",
    "        \"sec-ch-ua-mobile\":\"?0\",\n",
    "        \"sec-ch-ua-platform\":\"Linux\",\n",
    "        \"sec-fetch-dest\":\"document\",\n",
    "        \"sec-fetch-mode\":\"navigate\",\n",
    "        \"sec-fetch-site\":\"same-origin\",\n",
    "        \"sec-fetch-user\":\"?1\",\n",
    "        \"upgrade-insecure-requests\":\"1\"\n",
    "            }\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            res = requests.get(base_url.format(observation_id), headers=headers)\n",
    "            print(f\"Requesting {base_url.format(observation_id)} (Attempt {attempt + 1})\")\n",
    "            res.raise_for_status()\n",
    "            soup = bs4.BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            observation = {\n",
    "                \"observation_id\": observation_id,\n",
    "                \"species_id\": soup.select_one(\"h1 > a\").get('href').split('/')[2],\n",
    "                \"species_name\": soup.select_one(\".species-common-name\").getText().strip(),\n",
    "                \"species_name_scientific\": soup.select_one(\".species-scientific-name\").getText().strip(),\n",
    "                \"validation\": soup.select_one(\".validation-status-text\").getText().strip(),\n",
    "                \"gps_coordinates\": soup.select_one('span[title=\"WGS 84\"] .teramap-coordinates-coords').getText().strip() if soup.select_one('span[title=\"WGS 84\"] .teramap-coordinates-coords') else None,\n",
    "                \"accuracy\": soup.find('span', string=\"Nauwkeurigheid\").next_sibling.getText().strip() if soup.find('span', string=\"Nauwkeurigheid\") else None,\n",
    "                \"source\": soup.find('span', string=\"Bron\").next_sibling.getText().strip() if soup.find('span', string=\"Bron\") else None,\n",
    "                \"date\": soup.select_one('th:-soup-contains(\"Datum\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Datum\") + td') else None,\n",
    "                \"amount\": soup.select_one('th:-soup-contains(\"Aantal\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Aantal\") + td') else None, # aantal\n",
    "                \"life_stage\": soup.select_one('th:-soup-contains(\"Levensstadium\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Levensstadium\") + td') else None, # levensstadium\n",
    "                \"activity\": soup.select_one('th:-soup-contains(\"Activiteit\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Activiteit\") + td') else None, # activiteit\n",
    "   \n",
    "                \"location_id\": soup.select_one('th:-soup-contains(\"Locatie\") + td').a.get('href').split('/')[2] if soup.select_one('th:-soup-contains(\"Locatie\") + td a') else None, \n",
    "                \"location\": soup.select_one('th:-soup-contains(\"Locatie\") + td a').getText().strip() if soup.select_one('th:-soup-contains(\"Locatie\") + td a') else None, # locatie\n",
    "                \"observer_id\": soup.select_one('th:-soup-contains(\"Waarnemer\") + td').a.get('href').split('/')[2] if soup.select_one('th:-soup-contains(\"Waarnemer\") + td a') else None,\n",
    "                \"observer_name\": soup.select_one('th:-soup-contains(\"Waarnemer\") + td a').getText().strip() if soup.select_one('th:-soup-contains(\"Waarnemer\") + td a') else None, # waarnemer\n",
    "                \n",
    "                \"counting_method\": soup.select_one('th:-soup-contains(\"Telmethode\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Telmethode\") + td') else None, # telmethode\n",
    "                \"method\": soup.select_one('th:-soup-contains(\"Methode\") + td').getText().strip() if soup.select_one('th:-soup-contains(\"Methode\") + td') else None, # methode\n",
    "            }\n",
    "            # print(observation)\n",
    "            return observation\n",
    "        \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if res.status_code == 404:\n",
    "                print(f\"Observation {observation_id} not found (404). Returning empty observation.\")\n",
    "                return {\"observation_id\": observation_id, **{key: None for key in [\n",
    "                    \"species_id\", \"species_name\", \"species_name_scientific\", \"validation\",\n",
    "                    \"gps_coordinates\", \"accuracy\", \"source\", \"date\", \"amount\",\n",
    "                    \"life_stage\", \"activity\", \"location_id\", \"location\",\n",
    "                    \"observer_id\", \"observer_name\", \"counting_method\", \"method\"\n",
    "                ]}}\n",
    "            else:\n",
    "                print(f\"HTTP error {res.status_code} on attempt {attempt + 1}/{retries}: {e}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error: {e} on attempt {attempt + 1}/{retries}\")\n",
    "            \n",
    "        if attempt < retries - 1:\n",
    "            time.sleep(backoff_factor * (2 ** attempt))\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Failed to fetch observation {observation_id} after {retries} attempts.\")\n",
    "            raise e\n",
    "            \n",
    "def scrape(observations, species_id, folder_path, sleep_min = 2, sleep_max = 10):\n",
    "    file_name = make_filename(species_id, folder_path)\n",
    "    # print(f\"make_filename: {file_name}\")\n",
    "    \n",
    "    try:\n",
    "        for i, observation_id in enumerate(observations):\n",
    "            print(f\"Scraping observation {observation_id} | ({i+1:_}/{len(observations):_})\")\n",
    "            if (i == 0 and not os.path.isfile(file_name)):\n",
    "                write_header = True\n",
    "                # print(f\"Writing header to {file_name}\")\n",
    "            else:\n",
    "                write_header = False\n",
    "            \n",
    "            observation = parse_observation(observation_id)\n",
    "            pd.DataFrame([observation]).to_csv(\n",
    "                file_name,\n",
    "                mode='a',  # Append mode\n",
    "                index=False,\n",
    "                header=write_header  # Write header only for the first write in the file\n",
    "            )\n",
    "            \n",
    "            time.sleep(randint(sleep_min,sleep_max))  # Variable respectful delay between requests\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    \n",
    "def make_filename(species_id, folder_path, complete=False):\n",
    "    os.makedirs('scraped_data', exist_ok=True)\n",
    "    base_name = f'{folder_path}/observation_details_{species_id}_clean'\n",
    "    if not complete:\n",
    "        return base_name + '_in_progress.csv'\n",
    "    else:\n",
    "        return base_name + '.csv'\n",
    "\n",
    "###########################################################################################################################################\n",
    "species_id = 70\n",
    "folder_path_clean = \"./scraped_data/cleaned/\"\n",
    "scrape_amount = 20000\n",
    "sleep_min = 1\n",
    "sleep_max = 4\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "df_general = pd.read_csv(folder_path_clean + \"observations_general_\" + str(species_id) + \"_clean\" + \".csv\")\n",
    "details = [f for f in os.listdir(folder_path_clean) if f\"observation_details_{species_id}_clean\" in f and f.endswith(\".csv\")]\n",
    "\n",
    "df_details = pd.read_csv(folder_path_clean + details[0]) if len(details) > 0 else None\n",
    "\n",
    "if (df_details is not None):\n",
    "    # Find ids in df_general that are not in df_details\n",
    "    not_in_details = df_general[~df_general['id'].isin(df_details['observation_id'])]['id'].tolist()\n",
    "else:\n",
    "    not_in_details = df_general['id'].tolist()\n",
    "    \n",
    "\n",
    "if len(not_in_details) > 0:\n",
    "    print(f\"Start scraping: {scrape_amount:_} this batch, but {len(not_in_details):_} observations to scrape\")\n",
    "    scrape(not_in_details[0:scrape_amount], species_id, folder_path_clean, sleep_min, sleep_max)\n",
    "else:\n",
    "    print(\"All observations have been scraped.\")\n",
    "    os.rename(make_filename(species_id, folder_path_clean, complete=False), make_filename(species_id, folder_path_clean, complete=True))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "end_time = datetime.now()\n",
    "execution_time = end_time - start_time\n",
    "days, rem = divmod(execution_time.total_seconds(), 86400)\n",
    "hours, rem = divmod(rem, 3600)\n",
    "minutes, rem = divmod(rem, 60)\n",
    "seconds, _ = divmod(rem, 1)\n",
    "print(f\"Execution time: {int(days)} days, {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
